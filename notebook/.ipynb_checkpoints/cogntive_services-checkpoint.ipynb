{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Services for Computer Vision - Facial Recognition\n",
    "### Talk in TDC SP 2019 Event<br>Track: Cognitive Computing<br>Date: Jun, 16 2019\n",
    "\n",
    "**Comparison between four vendors:**\n",
    "- Amazon Rekognition\n",
    "- Microsoft Face API\n",
    "- IBM Watson Visual Recognition\n",
    "- Chooch\n",
    "\n",
    "**After this comparison, you'll be able to:**\n",
    "- Store images in a \"database\" for using in facial recognition\n",
    "- \"Train\" model with faces database, generating face embeddings (faceprints)\n",
    "- Run face recognition on a new image\n",
    "- Draw Bounding Boxes for face matches, with info (confidence/accuracy, name)\n",
    "- Draw Bounding Boxes for face unmatches, with \"unknown label\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start ... Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import boto3 # AWS SDK\n",
    "import cognitive_face as CF # Microsoft Face API SDK\n",
    "from ibm_watson import VisualRecognitionV3 as VR # IBM Visual Recognition SDK\n",
    "import cv2 # Open CV\n",
    "from imutils import paths # Image utils from pyimagesearch (Adrian Rosebrock)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import collections\n",
    "import time\n",
    "import zipfile\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import configparser\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2bytes(filename):\n",
    "    \"\"\" \n",
    "    Convert a imagefile in bytes\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Image file to convert (with relative path to notebook folder)\n",
    "\n",
    "    Returns:\n",
    "    bytes: Array of bytes from image file\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    with open(filename, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    \n",
    "    return image_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config_file(config_file):\n",
    "    \"\"\" \n",
    "    Read config file with API Keys for Cognitive Services\n",
    "\n",
    "    Parameters:\n",
    "    config_file (str): Config file to read (inside config folder of this project)\n",
    "\n",
    "    Returns:\n",
    "    config (dict) - Dictionary for Config File\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    config = configparser.RawConfigParser()\n",
    "    config.read('../config/' + config_file)  \n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_section(config):\n",
    "    \"\"\" \n",
    "    Return config sections populated by read_config_file function\n",
    "\n",
    "    Parameters:\n",
    "    config (dict) - Config sections\n",
    "\n",
    "    Returns:\n",
    "    config_key (str) - Content of specific key in specific section at config file\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hasattr(get_config_section, 'section_dict'):\n",
    "        get_config_section.section_dict = dict()\n",
    "        for section in config.sections():\n",
    "            get_config_section.section_dict[section] = dict(config.items(section))\n",
    "            \n",
    "    return get_config_section.section_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zipfiles(dataset):\n",
    "    \n",
    "    \"\"\" \n",
    "    Create Zip files for each folder (Person) in dataset\n",
    "\n",
    "    Parameters:\n",
    "    dataset (str): Folder containing image dataset, relative to this project folder\n",
    "\n",
    "    Returns:\n",
    "    subdirs (array): List of subdirs (Person Names)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    start_ts = time.time()\n",
    "\n",
    "    subdirs = [o for o in os.listdir('../' + dataset) if os.path.isdir(os.path.join('../' + dataset,o))]\n",
    "    \n",
    "    for name in subdirs:\n",
    "        zf = zipfile.ZipFile('../' + dataset + \"/\" + name + \".zip\", \"w\")\n",
    "        files = [x for x in os.listdir('../' + dataset + \"/\" + name)]\n",
    "        for filename in files:\n",
    "            zf.write(os.path.join('../' + dataset + \"/\" + name, filename), filename)\n",
    "        zf.close()\n",
    "        \n",
    "    print(f\"Zipping time: {time.time()-start_ts} seconds\")\n",
    "        \n",
    "    return subdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Config Files\n",
    "config = read_config_file('config.properties')\n",
    "config_dict = get_config_section(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for this experiment\n",
    "person_group = 'persongrouptest'          # Person Group Id for Microsoft Face API\n",
    "person_group_name = 'Person Group Test'   # Person Group Name for Microsoft Face API\n",
    "collection_id = 'rektest01'               # Collection Id for Amazon Rekognition\n",
    "classifier_name = 'faces_classifier'      # Classifier Name for IBM Watson Visual Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Store images for using in a facial recognition task\n",
    "\n",
    "Collect pictures with faces, label them with person names and finally store in database, bucket or storage, in a way it can possible to apply some embeddding / encoding algorithm to transform this face in a faceprint.\n",
    "\n",
    "**Amazon Rekognition:**\n",
    "- Store images in a S3 Bucket, label them with person name, using metadata field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_aws():\n",
    "    \"\"\" \n",
    "    Configure access to AWS Services\n",
    "\n",
    "    Parameters:\n",
    "    none\n",
    "\n",
    "    Returns:\n",
    "    client_rekognition (boto3.client): Client for Amazon Rekognition\n",
    "    client_s3 (boto3.client): Client for AWS S3\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create client of boto3 (AWS SDK) for Amazon Rekognition\n",
    "    client_rekognition = boto3.client(\n",
    "        \"rekognition\", \n",
    "        aws_access_key_id=config_dict['Amazon']['access_key_id'],\n",
    "        aws_secret_access_key=config_dict['Amazon']['secret_access_key'],\n",
    "        region_name=config_dict['Amazon']['region_name']\n",
    "    )\n",
    "    \n",
    "    # Create client of boto3 (AWS SDK) for AWS S3\n",
    "    client_s3 = boto3.client(\n",
    "        \"s3\", \n",
    "        aws_access_key_id=config_dict['Amazon']['access_key_id'],\n",
    "        aws_secret_access_key=config_dict['Amazon']['secret_access_key'],\n",
    "        region_name=config_dict['Amazon']['region_name']\n",
    "    )\n",
    "    \n",
    "    return client_rekognition, client_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_images_s3(dataset, client_s3, bucket_s3):\n",
    "    \"\"\"\n",
    "    Read a image dataset and store in AWS S3 Bucket\n",
    "\n",
    "    Parameters:\n",
    "    dataset (str): Folder containing image dataset, relative to this project folder\n",
    "    client_s3 (boto3.client): AWS S3 Client\n",
    "    bucket_s3 (str): Name of bucket inside AWS S3 Service\n",
    "\n",
    "    Returns:\n",
    "    none\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    start_ts = time.time()\n",
    "    \n",
    "    # list folders with person names, inside dataset folder\n",
    "    imagePaths = list(paths.list_images('../' + dataset))\n",
    "    \n",
    "    # iterate into images dataset\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        \n",
    "        print(\"Storing image {}/{} in S3\".format(i + 1, len(imagePaths)))\n",
    "        # extract the person name from the image path\n",
    "        personName = imagePath.split(os.path.sep)[-2]\n",
    "        fileName = imagePath.split(os.path.sep)[-1]\n",
    "        imageFile = open(imagePath,'rb')\n",
    "        \n",
    "        # Call AWS S3 Function to Store Image in Bucket\n",
    "        ret = client_s3.put_object(Bucket=bucket_s3, \n",
    "            Key=personName+'-'+fileName, \n",
    "            Body=imageFile,\n",
    "            Metadata={'FullName':personName}\n",
    "        )\n",
    "    \n",
    "    print(f\"Storing time: {time.time()-start_ts} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code for store images - AWS\n",
    "rekognition, s3 = configure_aws()\n",
    "store_images_s3('dataset', s3, config_dict['Amazon']['bucket_s3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Microsoft Face API:**\n",
    "- Create Person Group object to store all Persons to be identified\n",
    "- Create Person object into Person Group\n",
    "- Detect Face in each image\n",
    "- Add images into Person Group and Person, using bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_faceapi():\n",
    "    \n",
    "    \"\"\" \n",
    "    Configure access to Microsoft Face API\n",
    "\n",
    "    Parameters:\n",
    "    none\n",
    "\n",
    "    Returns:\n",
    "    none\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set Key to SDK\n",
    "    KEY = config_dict['Microsoft']['key']\n",
    "    CF.Key.set(KEY)\n",
    "\n",
    "    # Set BASE_URL to SDK    \n",
    "    BASE_URL = config_dict['Microsoft']['base_url']\n",
    "    CF.BaseUrl.set(BASE_URL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_images_azure(dataset, person_group, person_group_desc):\n",
    "    \"\"\"\n",
    "    Read a image dataset and store in Azure Face API Service\n",
    "\n",
    "    Parameters:\n",
    "    dataset (str): Folder containing image dataset, relative to this project folder\n",
    "    person_group (str): Code / Mnemonic for Person Group\n",
    "    person_group_desc (str): Description of Person Group\n",
    "\n",
    "    Returns:\n",
    "    none\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    start_ts = time.time()\n",
    "\n",
    "    # Create person Group\n",
    "    CF.person_group.create(person_group, person_group_desc)\n",
    "\n",
    "    # list folders with person names, inside dataset folder\n",
    "    imagePaths = list(paths.list_images('../' + dataset))\n",
    "    \n",
    "    currentPerson = None\n",
    "    \n",
    "    # iterate into images dataset\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        print(\"Storing image metadata {}/{} in Azure\".format(i + 1, len(imagePaths)))\n",
    "        # extract the person name from the image path\n",
    "        personName = imagePath.split(os.path.sep)[-2]\n",
    "        \n",
    "        if personName != currentPerson:\n",
    "            person_id = CF.person.create(person_group, personName)\n",
    "            currentPerson = personName\n",
    "        \n",
    "        faces = CF.face.detect(imagePath)\n",
    "        if len(faces) > 0:\n",
    "            top = faces[0]['faceRectangle']['top']\n",
    "            left = faces[0]['faceRectangle']['left']\n",
    "            width = faces[0]['faceRectangle']['width']\n",
    "            height = faces[0]['faceRectangle']['height']\n",
    "            \n",
    "            bounding_box = str(left)+\",\"+str(top)+\",\"+str(width)+\",\"+str(height)\n",
    "                        \n",
    "            CF.person.add_face(imagePath, person_group, person_id['personId'], target_face=bounding_box)\n",
    "        \n",
    "        time.sleep(6)\n",
    "                \n",
    "    print(f\"Storing time: {time.time()-start_ts} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code for store images - Microsoft Azure\n",
    "configure_faceapi()\n",
    "store_images_azure('dataset', person_group, person_group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IBM Watson Visual Recognition:**\n",
    "- Create Zip file for each Person in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_watson():\n",
    "    \"\"\" \n",
    "    Configure access to IBM Watson Visual Recognition Service\n",
    "\n",
    "    Parameters:\n",
    "    none\n",
    "\n",
    "    Returns:\n",
    "    visual_recognition (VisualRecognitionV3): Client for IBM Watson Recognition\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create Visual Recognition Service\n",
    "    visual_recognition = VR(\n",
    "        version=config_dict['IBM']['version'],\n",
    "        iam_apikey=config_dict['IBM']['iam_api_key']\n",
    "    )\n",
    "    return visual_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code for store images (Creating Local Zip Files) - IBM\n",
    "client_watson = configure_watson()\n",
    "person_names_watson = create_zipfiles('dataset_ibm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chooch**\n",
    "\n",
    "- Create Person in Catalog\n",
    "- Add Images into Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_chooch():\n",
    "    \"\"\" \n",
    "    Configure access to Chooch\n",
    "\n",
    "    Parameters:\n",
    "    none\n",
    "\n",
    "    Returns:\n",
    "    api_key (str): API Key for Chooch\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    api_key = config_dict['Chooch']['api_key']\n",
    "    \n",
    "    return api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_images_chooch(api_key, dataset, model_id):\n",
    "    \"\"\"\n",
    "    Read a image dataset and store in Chooch Image Dataset\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): API Key for access Chooch\n",
    "    dataset (str): Folder containing image dataset, relative to this project folder\n",
    "    model_id (int): Model ID created in Chooch web interface \n",
    "\n",
    "    Returns:\n",
    "    none\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    start_ts = time.time()\n",
    "    \n",
    "    # list folders with person names, inside dataset folder\n",
    "    imagePaths = list(paths.list_images('../' + dataset))\n",
    "    \n",
    "    currentPerson = None\n",
    "    \n",
    "    # iterate into images dataset\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        print(\"Storing image metadata {}/{} in Chooch\".format(i + 1, len(imagePaths)))\n",
    "        # extract the person name from the image path\n",
    "        personName = imagePath.split(os.path.sep)[-2]\n",
    "        \n",
    "        if personName != currentPerson:\n",
    "            # Create Person\n",
    "            url_create_person = 'https://api.chooch.ai/predict/face?person_name='+personName+'&model_id='+str(model_id)+'&apikey='+api_key+'&command=create_person'\n",
    "            response = requests.post(url_create_person)\n",
    "            response_create_person = json.loads(response.content)\n",
    "                        \n",
    "            if (response_create_person['status_description'] == 'success'):\n",
    "                person_id = response_create_person['person_id']\n",
    "                currentPerson = personName\n",
    "\n",
    "        # Add Image to Person\n",
    "        url_add_image = 'https://api.chooch.ai/predict/face?person_id_filter='+str(person_id)+'&apikey='+api_key+'&command=insert_person_image'\n",
    "        image_file = {'image': open(imagePath, 'rb')}\n",
    "        response = requests.post(url_add_image, files=image_file)\n",
    "        response_add_image = json.loads(response.content)\n",
    "                                        \n",
    "    print(f\"Storing time: {time.time()-start_ts} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code for store images - Chooch\n",
    "api_key = configure_chooch()\n",
    "store_images_chooch(api_key, 'dataset', config_dict['Chooch']['model_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. \"Train\" model with faces database, generating face embeddings (faceprints)\n",
    "\n",
    "Read a collection of images from storage and generate embeddding / encoding algorithm to transform this face in a faceprint.\n",
    "\n",
    "**Amazon Rekognition:**\n",
    "- Create a collection in rekognition, which generates embedding about each face detected in each image from bucket\n",
    "- Store Face ID generated by embedding task as key with label (person name) from image in a key-value database (DynamoDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rekognition(client_rekognition, client_s3, bucket_s3, collection):\n",
    "    \"\"\"\n",
    "    Train a image dataset, generating faces index\n",
    "\n",
    "    Parameters:\n",
    "    client_rekognition (boto3.client): Client for Amazon Rekognition\n",
    "    client_s3 (boto3.client): Client for AWS S3\n",
    "    bucket_s3 (str): Bucket name in AWS S3\n",
    "    collection (str): Id for Collection for Face Indexes\n",
    "\n",
    "    Returns:\n",
    "    none\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    start_ts = time.time()\n",
    "    \n",
    "    # create collection on Rekognition\n",
    "    print(\"Creating collection in Rekognition\")\n",
    "    client_rekognition.create_collection(CollectionId=collection)\n",
    "    \n",
    "    # get list of images from S3\n",
    "    print(\"Get list of images from S3\")\n",
    "    imageList = client_s3.list_objects_v2(\n",
    "        Bucket=bucket_s3,\n",
    "    )\n",
    "    \n",
    "    # for each file from S3\n",
    "    for (i, file) in enumerate(imageList['Contents']):\n",
    "        print(\"Embedding image {}/{} from S3\".format(i + 1, len(file) + 1))\n",
    "        personName = file['Key'].split('-')[-2]\n",
    "        # generate embeddings using IndexFaces method\n",
    "        face_embedding = client_rekognition.index_faces(\n",
    "                            CollectionId=collection,\n",
    "                            Image={\n",
    "                                'S3Object': {\n",
    "                                    'Bucket': bucket_s3,\n",
    "                                    'Name': file['Key']\n",
    "                                }\n",
    "                            },\n",
    "                            ExternalImageId=personName,\n",
    "                            DetectionAttributes=['DEFAULT'],\n",
    "                            MaxFaces=1\n",
    "                        )\n",
    "        \n",
    "    print(f\"Training time: {time.time()-start_ts} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code for embedding - AWS\n",
    "train_rekognition(rekognition, s3, config_dict['Amazon']['bucket_s3'], collection_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Microsoft Face API:**\n",
    "- Train model in Person Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_faceapi(person_group):\n",
    "    \"\"\"\n",
    "    Train a Person Group\n",
    "\n",
    "    Parameters:\n",
    "    person_group (str): Name of Person Group\n",
    "\n",
    "    Returns:\n",
    "    none\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    start_ts = time.time()\n",
    "    \n",
    "    CF.person_group.train(person_group)\n",
    "    \n",
    "    i = 1\n",
    "    status = 'Pending'\n",
    "    \n",
    "    while (status != 'succeeded'):\n",
    "        print(\"Training - Step {}\".format(i))\n",
    "        response = CF.person_group.get_status(person_group)\n",
    "        status = response['status']\n",
    "        i =+ 1\n",
    "        \n",
    "    print(f\"Training time: {time.time()-start_ts} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code for training - Microsoft Face API\n",
    "train_faceapi(person_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IBM Watson Visual Recognition:**\n",
    "- Create a Classifier with images, naming each Classifier with Person Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_watson_classifier(client, person_names, dataset, classifier):\n",
    "    \"\"\"\n",
    "    Create and train a image classifier, using dataset\n",
    "\n",
    "    Parameters:\n",
    "    client (VisualRecognitionV3): Client for IBM Watson Visual Recognition\n",
    "    names (str): Array of Person Names, obtained in dataset subfolders\n",
    "    dataset (str): Folder containing image dataset, relative to this project folder\n",
    "    classifier (str): Name of Classifier\n",
    "\n",
    "    Returns:\n",
    "    model (json): Information about new classifier. https://cloud.ibm.com/apidocs/visual-recognition#create-a-classifier\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    start_ts = time.time()\n",
    "    \n",
    "    positiveClasses = {}\n",
    "    \n",
    "    names = [x for x in person_names if x != 'negative']\n",
    "    \n",
    "    # Open ZIP for positive classes (Person Names)\n",
    "    for person in names:\n",
    "        print('Creating Dict for Person Name: {}'.format(person))\n",
    "        positiveClasses[person] = open('../' + dataset + '/' + person + '.zip', 'rb')\n",
    "        \n",
    "    # Open ZIP for negative class\n",
    "    negativeClasses = open('../' + dataset + '/negative.zip', 'rb')\n",
    "    \n",
    "    # Create \n",
    "    model = client.create_classifier(\n",
    "        name=classifier,\n",
    "        positive_examples=positiveClasses,\n",
    "        negative_examples=negativeClasses)\n",
    "        \n",
    "    print(f\"Training time: {time.time()-start_ts} seconds\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code for creating classifier - Watson Visual Recognition\n",
    "watson_classifier = create_watson_classifier(client_watson, person_names_watson, 'dataset_ibm', classifier_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watson_classifier_id = watson_classifier.result['classifier_id']\n",
    "print(watson_classifier_id)\n",
    "print(watson_classifier.result['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run face recognition on a new image\n",
    "\n",
    "With a new captured image, execute facial recognition, search new image in embeddings database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_known_face = '../test/T0004.jpg'\n",
    "new_image_unknown_face = '../test/u0001.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon Rekognition:**\n",
    "- Search for faces in a collection similar to face detected in the new image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_recognition_aws(client_rekognition, image, collection, threshold):\n",
    "    \"\"\"\n",
    "    Execute Face Recognition in Collection trained in Amazon Rekognition\n",
    "\n",
    "    Parameters:\n",
    "    client_rekognition (boto3.client): Client for Amazon Rekognition\n",
    "    image (str): Image file to recognize face (with relative path to notebook folder)\n",
    "    collection (str): Name of the trained collection\n",
    "    threshold (int): Threshold for Face Recognition\n",
    "\n",
    "    Returns:\n",
    "    personname (str): Person Name\n",
    "    bounding_box (dict): Bounding box with face detected in image. Format: Top, Left, Width, Height\n",
    "    similarity (float): Similarity level in percentage (accuracy)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    start_ts = time.time()\n",
    "    \n",
    "    # Search collection using image\n",
    "    known_faces = client_rekognition.search_faces_by_image(\n",
    "        CollectionId=collection,\n",
    "        Image={\n",
    "            'Bytes': image2bytes(image)\n",
    "        },\n",
    "        FaceMatchThreshold=threshold\n",
    "    )\n",
    "    \n",
    "    img1 = cv2.imread(image)\n",
    "    imgHeight, imgWidth, _ = img1.shape\n",
    "    \n",
    "    box = known_faces['SearchedFaceBoundingBox']\n",
    "    \n",
    "    left = int(imgWidth * box['Left'])\n",
    "    top = int(imgHeight * box['Top'])\n",
    "    width = int(imgWidth * box['Width'])\n",
    "    height = int(imgHeight * box['Height'])\n",
    "    \n",
    "    bounding_box = {'Top': top, 'Left': left, 'Width': width, 'Height': height}\n",
    "    \n",
    "    if len(known_faces['FaceMatches']) > 0:\n",
    "        personname = known_faces['FaceMatches'][0]['Face']['ExternalImageId']\n",
    "        similarity = known_faces['FaceMatches'][0]['Similarity']\n",
    "    else:   \n",
    "        personname = 'Unknown'\n",
    "        similarity = 0.0\n",
    "        \n",
    "    print(f\"Recognition time: {time.time()-start_ts} seconds\")\n",
    "    \n",
    "    return personname, bounding_box, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run facial recognition (Known Face) - AWS\n",
    "personname_aws, bounding_box_aws, similarity_aws = face_recognition_aws(rekognition, new_image_known_face, collection_id, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run facial recognition (Unknown Face) - AWS\n",
    "personname_aws_u, bounding_box_aws_u, similarity_aws_u = face_recognition_aws(rekognition, new_image_unknown_face, collection_id, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Microsoft Face API:**\n",
    "- Run face detect and identify using data stored and trained in Person Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_recognition_microsoft(person_group, image, threshold):\n",
    "    \"\"\"\n",
    "    Execute Face Recognition in Person Group defined in Azure Cognitive Face API\n",
    "\n",
    "    Parameters:\n",
    "    person_group (str): Name of Person Group\n",
    "    image (str): Image file to recognize face (with relative path to notebook folder)\n",
    "    threshold (int): Threshold for Face Recognition\n",
    "\n",
    "    Returns:\n",
    "    personname (str): Person Name\n",
    "    bounding_box (dict): Bounding box with face detected in image. Format: Top, Left, Width, Height\n",
    "    confidence (float): Confidence level in percentage (accuracy)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    start_ts = time.time()\n",
    "\n",
    "    # Run Face Detection with new image\n",
    "    faces = CF.face.detect(image)\n",
    "    face_ids = [f['faceId'] for f in faces]\n",
    "    \n",
    "    if len(faces) > 0:\n",
    "        top = faces[0]['faceRectangle']['top']\n",
    "        left = faces[0]['faceRectangle']['left']\n",
    "        width = faces[0]['faceRectangle']['width']\n",
    "        height = faces[0]['faceRectangle']['height']\n",
    "\n",
    "        bounding_box = {'Top': top, 'Left': left, 'Width': width, 'Height': height}\n",
    "    \n",
    "        face_matches = CF.face.identify(face_ids, person_group)\n",
    "        \n",
    "        if len(face_matches[0]['candidates']) > 0:\n",
    "            person = CF.person.get(person_group, face_matches[0]['candidates'][0]['personId'])\n",
    "            personname = person['name']\n",
    "            confidence = face_matches[0]['candidates'][0]['confidence']\n",
    "        else:\n",
    "            personname = 'Unknown'\n",
    "            confidence = 0.0\n",
    "            \n",
    "    print(f\"Recognition time: {time.time()-start_ts} seconds\")\n",
    "    \n",
    "    return personname, bounding_box, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run facial recognition (Known Face) - Microsoft Face API\n",
    "personname_microsoft, bounding_box_microsoft, confidence_microsoft = face_recognition_microsoft(person_group, new_image_known_face, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run facial recognition (Unknown Face) - Microsoft Face API\n",
    "personname_microsoft_u, bounding_box_microsoft_u, confidence_microsoft_u = face_recognition_microsoft(person_group, new_image_unknown_face, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IBM Watson Visual Recognition:**\n",
    "- Run face detection on new photo\n",
    "- Classify new image, using classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection_watson(client, image):\n",
    "    \"\"\"\n",
    "    Detect Faces in a image, using IBM Watson Visual Recognition\n",
    "\n",
    "    Parameters:\n",
    "    client (VisualRecognitionV3): Client for IBM Watson Visual Recognition\n",
    "    image (str): Image file to recognize face (with relative path to notebook folder)\n",
    "\n",
    "    Returns:\n",
    "    bounding_box (dict): Bounding box with face detected in image. Format: Top, Left, Width, Height\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    start_ts = time.time()\n",
    "    \n",
    "    # Run Face Detection with new image\n",
    "    \n",
    "    with open(image, 'rb') as images_file:\n",
    "        faces = client.detect_faces(images_file).get_result()\n",
    "    \n",
    "    if len(faces['images'][0]['faces']) > 0:\n",
    "        top = faces['images'][0]['faces'][0]['face_location']['top']\n",
    "        left = faces['images'][0]['faces'][0]['face_location']['left']\n",
    "        width = faces['images'][0]['faces'][0]['face_location']['width']\n",
    "        height = faces['images'][0]['faces'][0]['face_location']['height']\n",
    "\n",
    "        bounding_box = {'Top': top, 'Left': left, 'Width': width, 'Height': height}\n",
    "        \n",
    "    print(f\"Face Detection time: {time.time()-start_ts} seconds\")\n",
    "\n",
    "    return bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image_watson(client, classifier_id, image, threshold):\n",
    "    \"\"\"\n",
    "    Run Image Classifier from IBM Watson Visual Recognition, using image\n",
    "\n",
    "    Parameters:\n",
    "    client (VisualRecognitionV3): Client for IBM Watson Visual Recognition\n",
    "    classifier_id (str): Classifier Id returned from training step\n",
    "    image (str): Image file to recognize face (with relative path to notebook folder)\n",
    "    threshold (int): Threshold for Image Classification\n",
    "\n",
    "    Returns:\n",
    "    person (str): Person Name\n",
    "    bounding_box (dict): Bounding box with face detected in image. Format: Top, Left, Width, Height\n",
    "    score (float): Score level in percentage (accuracy)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    start_ts = time.time()\n",
    "\n",
    "    # Run Classifier against new image\n",
    "    threshold = threshold / 100\n",
    "    \n",
    "    with open(image, 'rb') as images_file:\n",
    "        faces = client.classify(\n",
    "            images_file,\n",
    "            threshold='0.9',\n",
    "            classifier_ids=[classifier_id]).get_result()\n",
    "        \n",
    "    if len(faces['images'][0]['classifiers'][0]['classes']) > 0:\n",
    "        person = faces['images'][0]['classifiers'][0]['classes'][0]['class']\n",
    "        score = faces['images'][0]['classifiers'][0]['classes'][0]['score']\n",
    "    else:\n",
    "        person = 'Unknown'\n",
    "        score = 0.0\n",
    "        \n",
    "    print(f\"Recognition time: {time.time()-start_ts} seconds\")\n",
    "    \n",
    "    return person, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run facial detection (Known Face) - IBM Watson\n",
    "bounding_box_watson = face_detection_watson(client_watson, new_image_known_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run visual recognition (Known Face) - IBM Watson\n",
    "personname_watson, score_watson = classify_image_watson(client_watson, watson_classifier_id, new_image_known_face, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run facial detection (Unknown Face) - IBM Watson\n",
    "bounding_box_watson_u = face_detection_watson(client_watson, new_image_unknown_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run visual recognition (Unknown Face) - IBM Watson\n",
    "personname_watson_u, score_watson_u = classify_image_watson(client_watson, watson_classifier_id, new_image_unknown_face, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chooch:**\n",
    "\n",
    "- Run face recognition using data stored and trained in Perception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_recognition_chooch(api_key, image, model_id):\n",
    "    \"\"\"\n",
    "    Execute Face Recognition in Perception defined in Chooch Web App\n",
    "    \n",
    "    Parameters:\n",
    "    api_key (str): API Key for access Chooch\n",
    "    image (str): Image file to recognize face (with relative path to notebook folder)\n",
    "    model_id (int): Model ID created in Chooch web interface \n",
    "\n",
    "    Returns:\n",
    "    personname (str): Person Name\n",
    "    bounding_box (dict): Bounding box with face detected in image. Format: Top, Left, Width, Height\n",
    "    Similarity (float): Similarity level in percentage (accuracy)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    start_ts = time.time()\n",
    "    \n",
    "    # Run Face Recognition with new image\n",
    "    url_face_recognition = 'https://api.chooch.ai/predict/face?model_id='+str(model_id)+'&apikey='+api_key\n",
    "    image_file = {'image': open(image, 'rb')}\n",
    "    response = requests.post(url_face_recognition, files=image_file)\n",
    "    response_face_recognition = json.loads(response.content)\n",
    "    \n",
    "    X1, X2, Y1, Y2 = response_face_recognition['faces'][0]['coordinates'].split(',')\n",
    "    bounding_box = {'Top': int(Y1), 'Left': int(X1), 'Width': int(X2)-int(X1), 'Height': int(Y2)-int(Y1)}\n",
    "\n",
    "    if response_face_recognition['face_recog_hit']:\n",
    "        personname = response_face_recognition['faces'][0]['person_name']\n",
    "        similarity = response_face_recognition['faces'][0]['similarity']\n",
    "    else:\n",
    "        personname = 'Unknown'\n",
    "        similarity = 0.0\n",
    "        \n",
    "    print(f\"Recognition time: {time.time()-start_ts} seconds\")\n",
    "    \n",
    "    return personname, bounding_box, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run facial recognition (Known Face) - Chooch\n",
    "personname_chooch, bounding_box_chooch, similarity_chooch = face_recognition_chooch(api_key, new_image_known_face, config_dict['Chooch']['model_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run facial recognition (Unknown Face) - Chooch\n",
    "personname_chooch_u, bounding_box_chooch_u, similarity_chooch_u = face_recognition_chooch(api_key, new_image_known_face, config_dict['Chooch']['model_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Draw Bounding Boxes for face matches, with info (confidence/accuracy, name)\n",
    "\n",
    "Read data from facial recognition, check face matches and draw a bounding box with name and confidence/accuracy in the new image\n",
    "\n",
    "**Amazon / Microsoft / IBM / Chooch:**\n",
    "- Use a helper function (OpenCV) to draw a bounding box, passing facial recognition data, image and box info (Person Name and Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(box, personname, accuracy, image):\n",
    "    \"\"\"\n",
    "    Draw Bounding Box in image, with Person Name or Unknown and Accuracy Level\n",
    "    \n",
    "    Parameters:\n",
    "    bounding_box (dict): Bounding box with face detected in image. Format: Top, Left, Width, Height\n",
    "    personname (str): Person Name\n",
    "    accuracy (float): Accuracy level in percentage\n",
    "    image (str): Image file to draw bounding box (with relative path to notebook folder)\n",
    "\n",
    "    Returns:\n",
    "    none\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # display the image to our screen\n",
    "    img1 = cv2.imread(image)\n",
    "\n",
    "    left = box['Left']\n",
    "    top = box['Top']\n",
    "    width = box['Width']\n",
    "    height = box['Height']\n",
    "    \n",
    "    label = personname + '/' + str(accuracy)\n",
    "\n",
    "    cv2.rectangle(img1, (left, top), (left + width, top + height), (255, 0, 0), 5)\n",
    "    y = top - 15 if top - 15 > 15 else top + 15\n",
    "        \n",
    "    cv2.putText(img1, label, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 4.00, (255, 0, 0), 5)\n",
    "\n",
    "    plt.imshow(img1)\n",
    "    plt.title('Image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run draw bounding boxes (face matches) - AWS\n",
    "draw_bounding_boxes(bounding_box_aws, personname_aws, similarity_aws, new_image_known_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run draw bounding boxes (face matches) - Microsoft\n",
    "draw_bounding_boxes(bounding_box_microsoft, personname_microsoft, confidence_microsoft, new_image_known_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run draw bounding boxes (face matches) - Watson\n",
    "draw_bounding_boxes(bounding_box_watson, personname_watson, score_watson, new_image_known_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run draw bounding boxes (face matches) - Chooch\n",
    "draw_bounding_boxes(bounding_box_chooch, personname_chooch, similarity_chooch, new_image_known_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Draw Bounding Boxes for face unmatches, with unkwown tag\n",
    "\n",
    "Read data from facial recognition, check face unmatches and draw a bounding box with unknown tag in the new image\n",
    "\n",
    "**Amazon Rekognition:**\n",
    "- Use a helper function (OpenCV) to draw a bounding box, passing face recognition data, image and box info (Unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run draw bounding boxes (face unmatches) - AWS\n",
    "draw_bounding_boxes(bounding_box_aws_u, personname_aws_u, 0.0, new_image_unknown_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run draw bounding boxes (face unmatches) - Microsoft\n",
    "draw_bounding_boxes(bounding_box_microsoft_u, personname_microsoft_u, 0.0, new_image_unknown_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run draw bounding boxes (face unmatches) - Watson\n",
    "draw_bounding_boxes(bounding_box_watson_u, personname_watson_u, 0.0, new_image_unknown_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run draw bounding boxes (face unmatches) - Chooch\n",
    "draw_bounding_boxes(bounding_box_chooch_u, personname_chooch_u, 0.0, new_image_unknown_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful links that helped me build this notebook**\n",
    "\n",
    "- https://aws.amazon.com/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/\n",
    "- https://www.pyimagesearch.com/2019/03/25/building-a-raspberry-pi-security-camera-with-opencv/\n",
    "- https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rekognition.html\n",
    "- https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html\n",
    "- https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html\n",
    "- https://docs.microsoft.com/en-us/azure/cognitive-services/Face/Tutorials/FaceAPIinPythonTutorial\n",
    "- https://docs.microsoft.com/en-us/python/api/overview/azure/cognitiveservices/faceapi?view=azure-python\n",
    "- https://github.com/microsoft/Cognitive-Face-Python/\n",
    "- https://clemenssiebler.com/face-recognition-with-azure-cognitive-services-face-api/\n",
    "- https://cloud.ibm.com/apidocs/visual-recognition?code=python\n",
    "- https://chooch.ai/api/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
